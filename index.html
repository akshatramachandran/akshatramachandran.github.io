<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SVFDW0GK50"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SVFDW0GK50');
</script>


  <title>Akshat Ramachandran</title>
  
  <meta name="author" content="Akshat Ramachandran">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/gt_logo.png">
  <link href="https://fonts.googleapis.com/css2?family=Eczar:wght@400&display=swap" rel="stylesheet">

  <style>
    body {
      /* background-image: url('images/background6.jpeg'); */
      background-size: cover; /* Cover the entire page */
      background-position: center; /* Center the background image */
      background-repeat: no-repeat; /* Do not repeat the image */
    }
  </style>

</head>

<body>
  <table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
              <p style="text-align:center; font-family: 'Eczar', serif; font-size: 38px;">
                Akshat Ramachandran
            </p>
              <p class="justified">I am a 2nd year Ph.D student in Electrical and Computer Engineering (ECE) at the <a href="https://synergy.ece.gatech.edu">Synergy Lab</a> in the Georgia Institute of Technology (Georgia Tech) advised by <a href="https://tusharkrishna.ece.gatech.edu">Prof. Tushar Krishna</a>. </p>

              <p style="text-align:center">
              	<a href="https://scholar.google.com/citations?user=FiNleXIAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="data/Akshat_Ramachandran_CV_2025_new.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/akshat-ramachandran-3981951bb?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3BvyAcG4lASMSNVsDUS6vGNQ%3D%3D">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/ARamachandran2000">GitHub</a> &nbsp/&nbsp
                <a href="mailto:akshat.r@gatech.edu">Email</a>
              </p>

              <p class="justified">
              <b>Community Service:</b> In an effort to give back to the community, I conduct a weekly guidance session for undergraduate/graduate students. Please read the description in the form and if you feel this could benefit you, please fill out the <a href="https://forms.gle/JZUd3YvUA56ADK9fA">form</a>. (Note: On travel currently, please expect delays in response.)
              </p>


            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/akshat.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_photo.png" class="hoverZoomLink"></a>
            </td>
          </tr>

          <table style="width:101%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin:-25px 0px -25px 0px"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>  
              <p class="justified"> The central focus of my research is in the design and development of efficient and high performance algorithms, architectures and systems for accelerating emergent deep learning applications (computer vision and NLP). My research breaks down traditional barriers existing between different computing elements and adopts an interdisciplinary approach spanning the entire computing stack. To realise my vision of developing the next generation of computing systems, my interests and technical work spans a wide gamut and lies at the intersection of computer architecture, VLSI, computer arithmetic and deep learning.
               </p>
 
               <ul>
                <li><strong>Algorithms</strong>:<br>
                  <ul>
                    <li>Quantization: 
                      <em>[<a href="https://arxiv.org/abs/2403.0546">DAC'24</a>]</em>
                      <em>[<a href="https://arxiv.org/pdf/2407.05266">ECCV'24</a>]</em>
                      <em>[<a href="https://arxiv.org/pdf/2411.05282">ISCA'25</a>]</em><br>
                    </li>

                    <li>3D Vision:
                      <em>[<a href="https://ieeexplore.ieee.org/abstract/document/10222228">ICIP'23</a>]
                      [<a href="https://ieeexplore.ieee.org/abstract/document/9726086">ICONAT'22</a>]</em>
                    </li>
                    <li>Geoscience and Remote Sensing:
                      <em>[<a href="https://www.tandfonline.com/doi/abs/10.1080/27669645.2023.2202961">All Earth'23</a>]
                      [<a href="https://ieeexplore.ieee.org/abstract/document/10117227">SPIN'23</a>]</em><br>
                    </li>
                  </ul>
                </li>
              </ul> <!-- Close the first list here -->
              
              <ul> <!-- Start a new, separate list -->
                <li><strong>Computer Arithmetic</strong>:<br> <!-- Replace with your actual category -->
                  <ul>
                    <li>Posits: 
                      <em>
                      [<a href="https://arxiv.org/abs/2403.05465">DAC'24</a>]
                      [<a href="https://ieeexplore.ieee.org/abstract/document/9996725/">DSD'22</a>]</em>
                    </li>
                    <li>Microscaling format: 
                      <em>
                      [<a href="https://arxiv.org/pdf/2411.05282">ISCA'25</a>]</em>
                    </li>
                  </ul>
                </li>
              </ul>

              <ul> <!-- Start a new, separate list -->
                <li><strong>Architecture</strong>:<br> <!-- Replace with your actual category -->
                  <ul>
                    <li>Domain-Specific Architecture for Computer Vision: 
                      <em>
                      [<a href="https://arxiv.org/abs/2403.05465">DAC'24</a>]
                      [<a href="https://ieeexplore.ieee.org/abstract/document/9996725/">DSD'22</a>]
                      [<a href="https://arxiv.org/pdf/2411.05282">ISCA'25</a>]</em>
                    </li>

                    <li>AI for Hardware Design Space Exploration: 
                      <em>
                      [<a href="">DATE'25</a>]</em>
                    </li>

                  </ul>
                </li>
              </ul>
            </td>
          </tr> 
        </tbody></table>

          <table style="width:101%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>  
                <div style="height:200px;overflow-y:auto">
                <p></p><ul>
                  <li> [May. 2025] <b style="color:palevioletred">[Career]</b> Starting as a Research Intern at the ASIC and VLSI team in NVIDIA Research!  </li>                  
                  <li> [Apr. 2025] <b style="color:#3EA055;">[Paper]</b> Our work MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization has been accepted to <b>ISCA 2025!</b></li>
                  <li> [Apr. 2025] <b style="color: #3EA055;">[Presentation]</b> Presented our work on "<b> AIRCHITECT v2</b>" at DATE 2025 in Lyon, France </li>
                  <li> [Apr. 2025] <b style="color:blue;">[Talk]</b> I presented AIRCHITECT v2 at the <a href="https://eswml.github.io">ESWML Workshop</a>, co-located with ASPLOS 2025.</li>
                  <li> [Jan. 2025] <b style="color:black">[News]</b> Our research CLAMP-ViT is featured in CoCoSyS "Center Director's Research Highlights" </li>
                  <li> [Nov. 2024] <b style="color:#3EA055;">[Paper]</b> Our work AIRCHITECT v2: Learning the Hardware Accelerator Design Space through Unified Representations has been accepted to DATE 2025</li>
                  <li> [Nov. 2024] <b style="color:orchid;">[Award]</b> Our presentation of MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization received the second position at ACM Student Research Competition at MICRO.</li>
                  <li> [Jul. 2024] <b style="color:#3EA055;">[Paper]</b> Our work <a href="https://arxiv.org/pdf/2407.05266"><papertitle>CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs</papertitle></a> has been accepted to <strong>ECCV 2024</strong>.</li>
                  <li> [Jun. 2024] <b style="color: #3EA055;">[Presentation]</b> Presented our work on "<b> Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</b>" at DAC 2024 in San Francisco, CA, USA <a href="data/DAC2024_LPQ_Video_Presentation.mp4">[Video]</a> <a href="data/DAC2024_Poster_AR.pdf">[Poster]</a> </li>
                  <li> [Jun 2024] <b style="color: blue;">[Talk]</b> I gave a talk on <a href="data/05_29_24_LPQ_Presentation.pdf">Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</a> (DAC 2024) to IBM Research, NY. </li>
                  <li> [May 2024] <b style="color: blue;">[Talk]</b> I gave a talk on <a href="data/05_29_24_LPQ_Presentation.pdf">Quantization</a> to <a href="https://www.nvidia.com/en-us/">NVIDIA</a> GPU Power Architecture Team at Santa Clara, CA. </li>
                  <li> [May 2024] <b style="color: blue;">[Talk]</b> I gave a talk on <a href="data/05_29_24_LPQ_Presentation.pdf">Logarithmic Posits</a> to <a href="https://www.lemurianlabs.com">Lemurian Labs</a>. <a href="https://fathom.video/share/xn7fTbscErG_YPQsN3Zrsp33XkniWYQt?tab=summary">[Recording]</a> </li>
                  <li> [May 2024] <b style="color: palevioletred;">[Career]</b> Starting my internship as an Architecture Energy Modelling Intern at <a href="https://www.nvidia.com/en-us/">NVIDIA</a>, Santa Clara, California.</li>

                  <li> [Apr. 2024] <b style="color: orchid;">[Award]</b> Selected as <b>DAC Young Fellow</b> for DAC 2024. </li>

                  <li> [Apr. 2024] <b style="color: #3EA055;">[Presentation]</b> Presented our work on "<b> Algorithm-Hardware Co-Design for Data-Free Quantization and Inference using Contrastive Learning and Next Generation Arithmetic </b>" at the CoCoSyS Liaison Meeting. <a href="data/CoCoSyS Liaison Meeting Slides.pdf">(Slides)</a> </li>

                  <li> [Mar. 2024] <b style="color: orangered;">[Poster]</b> Presented our latest work <a href="https://arxiv.org/pdf/2403.05465.pdf"><papertitle>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings</papertitle></a> as a <a href="data/LPQ_Poster_AkshatR.pdf"><papertitle>poster</papertitle></a> at the <a href="https://www.linkedin.com/feed/update/urn:li:activity:7176000061892349954/">CoCoSys Annual Review Meeting</a> (March 12-13) in Atlanta, Georgia.</li>

                  <li> [Feb. 2024] <b style="color:#3EA055;">[Paper]</b> Our work <a href="https://arxiv.org/pdf/2403.05465.pdf"><papertitle>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</papertitle></a> has been accepted to <strong>DAC 2024</strong>.</li>

                  <li> [Oct. 2023] <b style="color:#3EA055;">[Paper Presentation]</b> Presented our work <a href="https://ieeexplore.ieee.org/abstract/document/10222228"><papertitle>Ntrans-Net: A Multi-Scale Neutrosophic-Uncertainty Guided Transformer Network For Indoor Depth Completion</papertitle></a> at ICIP 2023 in Kuala Lampur, Malaysia. <a href="data/icip_slides.pdf"><papertitle>[Slides]</papertitle></a></li>

                  <li> [Aug. 2023] <b style="color:red;">[Education]</b> Accepted to Georgia Tech and starting my graduate journey at the Synergy Lab.</li>

                  <li> [Jun. 2023] <b style="color: orchid;">[Award Nomination]</b> I am recognized in the top 5% of research and development in Samsung Research for my work on sparse ToF sensors towards the Samsung Best Paper Award.</li>
                  
                  <li> [Aug. 2022] <b style="color: palevioletred;">[Career]</b> Starting as a Visual Research Engineer at Samsung Research [Suwon, South Korea & Bangalore, India].</li>

                  <li> [Aug. 2022] <b style="color: orchid;">[Award]</b> I won the Best Undergraduate Thesis Award in the EE dept at Veermata Jijabai Technological Institute for my senior year thesis on ”Next Generation Architecture for Computer Vision”.</li>
                </ul>
          </div>
              </td>
            </tr>
          </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (Since 2024)</heading>  &nbsp &nbsp &nbsp (*: Equal Contributions)
            </td> 
          </tr> 
        </tbody></table> 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


                                        <!-- Copy from here -->
                                        <tr onmouseout="ff_stop()" onmouseover="ff_start()">
                                          <td style="padding:20px;width:25%;vertical-align:middle">
                                            <div class="one">
                                              <div class="two" id='ff_image'>
                                                <img src='images/microscopiq.png' class="aspect-ratio-img"></div>
                                              <img src='images/microscopiq.png' class="aspect-ratio-img">
                                            </div>
                                            <script type="text/javascript">
                                              function ff_start() {
                                                document.getElementById('ff_image').style.opacity = "1";
                                              }
                              
                                              function ff_stop() {
                                                document.getElementById('ff_image').style.opacity = "0";
                                              }
                                              ff_stop()
                                            </script>
                                          </td>
                            
                                          
                                          <td style="padding:20px;width:75%;vertical-align:middle">
                                                <papertitle>MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization</papertitle>
                                            <br>
                                            <strong>Akshat Ramachandran</strong>,
                                            <a href="https://scholar.google.com/citations?user=b591SVkAAAAJ&hl=en&oi=ao">Souvik Kundu</a>,
                                            <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
                                            <br>
                                            <em>International Symposium on Computer Architecture (<b>ISCA</b>), 2025</em>
                                            <br>
                                            <a href="https://arxiv.org/pdf/2411.05282">Paper</a>
                                            <br>
                                            <p></p>
                                            <p>Quantization of foundational models (FMs) is significantly more challenging than traditional DNNs due to the emergence of large magnitude features called outliers. Existing outlier-aware algorithm/architecture co-design techniques either use mixed-precision, retaining outliers at high precision but compromise hardware efficiency, or quantize inliers and outliers at the same precision, improving hardware efficiency at the cost of accuracy. To address this mutual exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique that leverages pruning to complement outlier-aware quantization. MicroScopiQ retains outliers at higher precision while pruning a certain fraction of least important weights to distribute the additional outlier bits; ensuring high accuracy, aligned memory and hardware efficiency. We design a high-throughput, low overhead accelerator architecture composed of simple multi-precision INT processing elements and a novel network-on-chip called ReCoN that efficiently abstracts the complexity of supporting high-precision outliers. Additionally, unlike existing alternatives, MicroScopiQ does not assume any locality of outlier weights, enabling applicability to a broad range of FMs. Extensive experiments across various quantization settings show that MicroScopiQ achieves SoTA quantization performance while simultaneously improving inference performance by 3x and reducing energy by 2x over existing alternatives.</p>
                                          </td>
                                          <!-- Copy till here -->


                                          <!-- Copy from here -->
                                          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
                                            <td style="padding:20px;width:25%;vertical-align:middle">
                                              <div class="one">
                                                <div class="two" id='ff_image'>
                                                  <img src='images/airchitect.png' class="aspect-ratio-img"></div>
                                                <img src='images/airchitect.png' class="aspect-ratio-img">
                                              </div>
                                              <script type="text/javascript">
                                                function ff_start() {
                                                  document.getElementById('ff_image').style.opacity = "1";
                                                }
                                
                                                function ff_stop() {
                                                  document.getElementById('ff_image').style.opacity = "0";
                                                }
                                                ff_stop()
                                              </script>
                                            </td>
                              
                                            
                                            <td style="padding:20px;width:75%;vertical-align:middle">
                                                  <papertitle>AIRCHITECT v2: Learning the Hardware Accelerator Design Space through Unified Representations</papertitle>
                                              <br>
                                              <strong>Akshat Ramachandran</strong>*,
                                              Jamin Seo*, Yu-Yuan Chang, Anirudh Itagi, Tushar Krishna
                                              <br>
                                              *Equal Contribution
                                              <br>
                                              <em>Design Automation and Test in Europe (<strong>DATE</strong>)</em>, 2025
                                              <br>
                                              <a href="https://arxiv.org/pdf/2501.09954">Paper</a>
                                              <br>
                                              <p></p>
                                              <p>Design space exploration (DSE) plays a crucial role in enabling custom hardware architectures, particularly for emerging applications like AI, where optimized and specialized designs are essential. With the growing complexity of deep neural networks (DNNs) and the introduction of advanced large language models (LLMs), the design space for DNN accelerators is expanding at an exponential rate. Additionally, this space is highly non-uniform and non-convex, making it increasingly difficult to navigate and optimize. Traditional DSE techniques rely on search-based methods, which involve iterative sampling of the design space to find the optimal solution. However, this process is both time-consuming and often fails to converge to the global optima for such design spaces. Recently, AIRCHITECT V1, the first attempt to address the limitations of search-based techniques, transformed DSE into a constant-time classification problem using recommendation networks. However, AIRCHITECT V1 lacked generalizability and had poor performance on complex design spaces. In this work, we propose AIRCHITECT V2, a more accurate and generalizable learning-based DSE technique applicable to large-scale design spaces that overcomes the shortcomings of earlier approaches. Specifically, we devise an encoder-decoder transformer model that (a) encodes the complex design space into a uniform intermediate representation using contrastive learning and (b) leverages a novel unified representation blending the advantages of classification and regression to effectively explore the large DSE space without sacrificing accuracy. Experimental results evaluated on 105 real DNN workloads demonstrate that, on average, AIRCHITECT V2 outperforms existing techniques by 15% in identifying optimal design points. Furthermore, to demonstrate the generalizability of our method, we evaluate performance on unseen model workloads and attain a 1.7x improvement in inference latency on the identified hardware architecture.</p>
                                            </td>
                                            <!-- Copy till here -->

                      <!-- Copy from here -->
                      <tr onmouseout="ff_stop()" onmouseover="ff_start()">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                          <div class="one">
                            <div class="two" id='ff_image'>
                              <img src='images/clamp_vit.png' class="aspect-ratio-img"></div>
                            <img src='images/clamp_vit.png' class="aspect-ratio-img">
                          </div>
                          <script type="text/javascript">
                            function ff_start() {
                              document.getElementById('ff_image').style.opacity = "1";
                            }
            
                            function ff_stop() {
                              document.getElementById('ff_image').style.opacity = "0";
                            }
                            ff_stop()
                          </script>
                        </td>
          
                        
                        <td style="padding:20px;width:75%;vertical-align:middle">
                              <papertitle>CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs</papertitle>
                          <br>
                          <strong>Akshat Ramachandran</strong>,
                          <a href="https://scholar.google.com/citations?user=b591SVkAAAAJ&hl=en&oi=ao">Souvik Kundu</a>,
                          <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
                          <br>
                          <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
                          <br>
                          <a href="https://arxiv.org/pdf/2407.05266">Paper</a>
                          <br>
                          <p></p>
                          <p>We present CLAMP-ViT, a data-free post-training quantization method for vision transformers (ViTs). We identify the limitations of recent techniques, notably their inability to leverage meaningful inter-patch relationships, leading to the generation of simplistic and semantically vague data, impacting quantization accuracy. CLAMP-ViT employs a two-stage approach, cyclically adapting between data generation and model quantization. Specifically, we incorporate a patch-level contrastive learning scheme to generate richer, semantically meaningful data. Furthermore, we leverage contrastive learning in layer-wise evolutionary search for fixed- and mixed-precision quantization to identify optimal quantization parameters while mitigating the effects of a non-smooth loss landscape. Extensive evaluations across various vision tasks demonstrate the superiority of CLAMP-ViT, with performance improvements of up to 3% in top-1 accuracy for classification, 0.6 mAP for object detection, and 1.5 mIoU for segmentation at similar or better compression ratio over existing alternatives.</p>
                        </td>
                        <!-- Copy till here -->

          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/DAC24.png' class="aspect-ratio-img"></div>
                <img src='images/DAC24.png' class="aspect-ratio-img">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</papertitle>
              <br>
              <strong>Akshat Ramachandran</strong>,
              <a href="https://zishenwan.github.io">Zishen Wan</a>,
              <a href="https://ghjeong12.github.io">Geonhwa Jeong</a>,
              <a href="http://www.johngustafson.net">John Gustafson</a>,
              <a href="https://scholar.google.com/citations?user=P__ztgcAAAAJ&hl=en">Tushar Krishna</a>
              <br>
              <em>ACM/IEEE Design Automation Conference (<strong>DAC</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2403.05465.pdf">Paper</a>
              <br>
              <p></p>
              <p>We present Logarithmic Posits (LP), a novel adaptive data type for Deep Neural Network (DNN) quantization, offering significant efficiency and accuracy improvements. Our approach, LP Quantization (LPQ), optimizes DNN parameters using a genetic algorithm, closely preserving model accuracy with a specialized objective. The resulting LP accelerator architecture (LPA) doubles performance per area and improves energy efficiency by 2.2x over traditional quantization methods, with minimal accuracy loss.</p>
            </td>
          </tr> 

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <li> <em>[2024]</em> Second position in ACM Student Research Competition at MICRO 2024.</li>
              <li> <em>[2024]</em> Recepient of MICRO 2024 Student Travel Grant.</li>  
              <li> <em>[2024]</em> Selected as <b>DAC Young Fellow Class</b> of 2024.</li>
              <li> <em>[2022]</em> Received the <b>Samsung Spot Award</b> for outstanding contribution towards design and development of AR/VR algorithms and acceleration on Samsung Galaxy devices.</li>

              <li><em>[2022]</em>Won the <b>Best Undergraduate Thesis Award</b> in the EE dept. for the senior year thesis on ”Next Generation Architecture for Computer Vision”.</li>
   				</ul>
              </td>
            </tr>
          </tbody></table>

          <!-- Experience -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>  </td>
                </tr>
                <tr>
                    <td width="100%" valign="middle">
                        <table>
                            <tbody>

                              <tr>
                                <td width="90%">
                                    <p><b>NVIDIA</b>, Santa Clara, CA, USA<br>
                                    ASIC and VLSI Research Intern
                                    • May 2025 to Aug 2025</p>
                                </td>
                                <td width="40%">
                                    <img src="./images/nvidia.png" height="50%" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                                </td>
                            </tr>

                              <tr>
                                <td width="90%">
                                    <p><b>Intel Labs</b>, Santa Clara, CA, USA<br>
                                    AI Hardware Research Intern
                                    • Aug 2024 to Dec 2024</p>
                                </td>
                                <td width="40%">
                                    <img src="./images/intel.png" height="50%" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                                </td>
                            </tr>

                              <tr>
                                    <td width="90%">
                                        <p><b>NVIDIA</b>, Santa Clara, CA, USA<br>
                                        Architecture Energy Modelling Intern
                                        • May 2024 to Aug 2024</p>
                                    </td>
                                    <td width="40%">
                                        <img src="./images/nvidia.png" height="50%" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                                    </td>
                                </tr>
                                <tr>
                                    <td width="90%">
                                        <p><b>Lemurian Labs</b>, Menlo Park, CA, USA<br>
                                        Hardware Intern (Remote)
                                        • Jun 2023 to Aug 2023</p>
                                    </td>
                                    <td width="40%">
                                        <img src="./images/lemurian.png" height="50%" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                                    </td>
                                </tr>
                                <tr>
                                    <td width="90%">
                                        <p><b>Samsung Research</b>, Suwon, South Korea & Bangalore, India<br>
                                        Visual Intelligence Research Engineer
                                        • Aug 2022 to Jul 2023</p>
                                    </td>
                                    <td width="20%">
                                        <img src="./images/samsung.jpeg" height="50%" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </td>
                </tr>
            </tbody>
        </table>
        <br>
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service</heading>  
 				<p></p><ul>
 				<li> <b>Conference Reviewer:</b> CoNGA 2024, ECCV 2024, SCOPE-ICLR 2025, IEEE COINS 2025.</li>
 				<li> <b>Journal Reviewer:</b> TCAD-Integrated Circuits and Systems</li>
      </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Activities and Picture Gallery</heading>  
 				<p></p><ul><li> Sports: I enjoy basketball, swimming, tennis and table tennis.</li>
 				<li> Music: I am a trained veena and violin player. <a href="https://www.youtube.com/watch?v=DUOeXIaMhJA">[YouTube]</a></li>
   				</ul>
              </td>
            </tr>

            <tr>
              <td colspan="4" style="text-align:center;">
                <table style="width:100%;border:0px;">
                  <tr>
                      <td><img src="images/CoCOSyS_group.JPEG" alt="Synergy@CoCoSyS" style="height:200px; width: 300px;"> <p>CoCoSyS Members</p></td>
                     
                      <td><img src="images/CoCoSys_QA.JPG" alt="Image 2" style="height:200px; width: 300px;">  <p>Deep Dive Session @ CoCoSyS</p></td>
                      <td><img src="images/Cocosys_talk.jpeg" alt="Image 3" style="height:200px; width: 300px;"> <p>Lightning Talk @CoCoSyS</p></td>
                      <td><img src="images/CoCoSyS_team.jpeg" alt="Image 4" style="height:200px; width: 300px;"><p>Synergy Lab@CoCoSyS</p></td>
                  </tr>
                  <tr>
                      <td><img src="images/Cape Cod.jpeg" alt="Image 5" style="height:200px; width: 300px;"><p>Hiking @ Long Beach</p></td>
                      <td><img src="images/Whales.png" alt="Image 6" style="height:200px; width: 300px;"> <p>Whale Watching @ Cape Cod</p></td>
                      <td><img src="images/Veena.jpeg" alt="Image 7" style="height:200px; width: 300px;"><p>Veena</p></td>
                      <td><img src="images/Basketball.jpeg" alt="Image 8" style="height:200px;"><p>State-Level Basketball Tournament</p></td>
                  </tr>
              </table>
              </td>
          </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last Update: Apr. 2025
                <br><br>
                <a href="https://github.com/zishenwan/zishenwan.github.io">Website Credits</a>
                </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>